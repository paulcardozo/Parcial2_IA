{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parcial2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEyqNZ9j19YeGk3oLiCU1w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulcardozo/Parcial2_IA/blob/master/Parcial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sago3zFG9hpA",
        "colab_type": "text"
      },
      "source": [
        "# 2. Realice un proceso completo de análisis usando pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dBxyIFL1eYH",
        "colab_type": "text"
      },
      "source": [
        "# Subir el dataset\n",
        "Haremos uso del dataset del repositorio de UCI respecto a tiempos de espera y la información para la respuesta de una cola de archivos que serán procesados en un servidor de arquitectura 2D-SOME Bus\n",
        "https://archive.ics.uci.edu/ml/datasets/Optical+Interconnection+Network+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD8FMV853wj6",
        "colab_type": "text"
      },
      "source": [
        "# Contenido Dataset\n",
        "\n",
        "\n",
        "*   Node Number: Se refiere a la cantidad de nodos utilizados en el servirdor para lograr el procesamiento del archivo solicitado\n",
        "*   Thread Number: Es el número de hilos creados para lograr el procesamiento\n",
        "*   Spatial Distribution: El rendimiento de la red utilizada según los siguientes modelos: Uniform (UN), Hot Region (HR), Bit reverse (BR) and Perfect Shuffle (PS)\n",
        "*   Temporal Distribution: Se refiere a la generación de los paquetes generados o recibidos por el servidor, estos pueden ser de tipo Client-Server o Asynchronous\n",
        "*   T/R: Tiempo de transferencia del mensaje medidos en ciclos de cómputo\n",
        "*   Processor Utilization: Un porcentaje de la cantidad utilizada del procesador\n",
        "*   Channel Waiting Time: El tiempo de espera de un paquete en el canal de salida que llega al servidor hasta ser ejecutado\n",
        "*   Input Waiting Time: El tiempo en que un paquete será ejecutado por el servidor\n",
        "*   Network Response Time: El tiempo que el paquete se encuentra dentro de la red del servidor\n",
        "*   Channel Utilization: El porcentaje del tamaño del canal que fue utilizado \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK3I9RoT2frK",
        "colab_type": "text"
      },
      "source": [
        "# Creación de Pipeline\n",
        "Se realiza la creación de un pipeline, enfocado en el método de StandardScaler para el preprocesamiento, que da la escala apropiada para las variables discretas, y para el proceso de análisis se realizará DecisionTreeClassifier, para lograr procesar todas las variables necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1mEcSbR2Uov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "6879baec-85f5-44fa-8ed6-a9423ab9b976"
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "# El dataset estará cargado en el framework de pandas\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/paulcardozo/Parcial2_IA/master/optical_interconnection_network.csv'\n",
        "dataset = pd.read_csv(url)\n",
        " \n",
        "cleanup_data = {\"Thread Number\":{4:0,8:1},\n",
        "        \"Spatial Distribution\":{\"UN\":0,\"HR\":1,\"BR\":2,\"PS\":3},\n",
        "        \"Temporal Distribution\":{\"Client-Server\":0,\"Asynchronous\":1}}\n",
        "dataset.replace(cleanup_data, inplace=True)\n",
        " \n",
        "#split dataset in features and target variable\n",
        "feature_cols = ['T/R', 'Temporal Distribution','Processor Utilization', 'Channel Waiting Time', 'Input Waiting Time','Network Response Time','Channel Utilization','Node Number']\n",
        "X = dataset[feature_cols] # Features\n",
        "y = dataset[\"Thread Number\"] # Target variable\n",
        " \n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
        " \n",
        " \n",
        "# Create Decision Tree classifer object\n",
        "clf = DecisionTreeClassifier()\n",
        " \n",
        "classifier = make_pipeline(StandardScaler(),clf)\n",
        "# Train Decision Tree Classifer\n",
        " \n",
        "classifier.fit(X_train,y_train)\n",
        "print(X_test)\n",
        "print()\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(y_pred)\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Precisión del Modelo:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     T/R  Temporal Distribution  ...  Channel Utilization  Node Number\n",
            "132  0.3                      0  ...             0.713385           64\n",
            "622  0.3                      1  ...             0.808437           16\n",
            "525  0.6                      1  ...             0.709583           16\n",
            "289  1.0                      1  ...             0.872309           64\n",
            "82   0.3                      0  ...             0.549167           64\n",
            "..   ...                    ...  ...                  ...          ...\n",
            "375  0.6                      0  ...             0.786875           16\n",
            "76   0.7                      0  ...             0.774557           64\n",
            "286  0.7                      1  ...             0.824323           64\n",
            "347  0.8                      0  ...             0.932778           16\n",
            "186  0.7                      1  ...             0.934853           64\n",
            "\n",
            "[192 rows x 8 columns]\n",
            "\n",
            "[ 6  1  0  0  0  0  0 10  1  1  6  0  1 10  1  6 10  6  1  6  6  0  6  1\n",
            "  0  0 10 10  0  1  1  6  1  6  0  1  0  1  6  0 10  1  0  6  0 10  0  6\n",
            "  6  6  6  1  6 10  1 10  6  1 10  0  0 10 10  1  1  0 10  1  0  1  1  0\n",
            " 10  0  0  0  1  6  1 10  0  0  1  6  6  0  0 10  6  1 10  0  6  6  6  1\n",
            "  6 10  6  6 10 10 10  0 10  6  6  0  6  1 10 10 10  0  0  6  1 10  6  0\n",
            "  1  1  0  0 10  6  1 10  0 10  0  6  1  6  1 10  6  0  6  0  0  1  0  6\n",
            "  6 10  6 10  1  6  0 10  0 10  6  0  1 10  0  0  6  1  0 10 10  1  1 10\n",
            " 10  6  0 10  6 10 10  1  1 10  6  0  1  0 10  1  6  1  1  6 10  0  1  1]\n",
            "Precisión del Modelo: 0.8385416666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-iwSOua3W5d",
        "colab_type": "text"
      },
      "source": [
        "# Resultado\n",
        "Cuando corremos el código del pipeline, sigue los procesos de preprocesamiento, a continuación del de análisis de datos. Con ello observamos en pantalla el dataset de prueba, y la predicción del número de hilos que se prevee utilizar según la implementación. También observamos la precisión de nuestro modelo con una precisión que ronda entre 80 y 85%"
      ]
    }
  ]
}